apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "spark-history-server.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "spark-history-server.name" . }}
    helm.sh/chart: {{ include "spark-history-server.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "spark-history-server.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "spark-history-server.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
        release: {{ .Release.Name }}
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      {{- range $key, $value := .Values.podAnnotations }}
        {{ $key }}: {{ $value | quote }}
      {{- end }}
    spec:
      {{- if .Values.global.sfScheduler.enabled }}
      schedulerName: {{ .Values.global.sfScheduler.schedulerName }}
      {{- end }}
      serviceAccountName: {{ include "spark-history-server.serviceAccountName" . }}
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        env:
        - name: HADOOP_CONF_DIR
          value: {{ .Values.hdfs.HADOOP_CONF_DIR }}
        - name: SPARK_NO_DAEMONIZE
          value: "true"
        ports:
        - name: historyport
          containerPort: 18080
          protocol: TCP
        resources:
{{ toYaml .Values.resources | indent 10 }}
        command:
        - "/bin/sh"
        - "-c"
        - >
          if [ "$enablePVC" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=file:/data/$eventsDir";
          elif [ "$enableGCS" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=gs://$logDirectory";
            if [ "$enableIAM" == "false" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/$key";
            fi;
          elif [ "$enableS3" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.history.fs.logDirectory=s3a://$logDirectory \
              -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem";
            if [ -n "$endpoint" ] && [ "$endpoint" != "default" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.hadoop.fs.s3a.endpoint=$endpoint";
            fi;
            if [ "$enableIAM" == "false" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.hadoop.fs.s3a.access.key=$awsAccessKeyID \
              -Dspark.hadoop.fs.s3a.secret.key=$awsSecretAccessKey";
            fi;
          elif [ "$enableWASBS" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
              -Dspark.history.fs.logDirectory=wasbs://$containerName@$storageAccountName.blob.core.windows.net/spark-hs \
              -Dspark.hadoop.fs.defaultFS=wasbs://$containerName@$storageAccountName.blob.core.windows.net \
              -Dspark.hadoop.fs.wasbs.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem \
              -Dspark.hadoop.fs.AbstractFileSystem.wasbs.impl=org.apache.hadoop.fs.azure.Wasbs";
            if [ "$sasKeyMode" == "true" ]; then
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
                -Dspark.hadoop.fs.azure.local.sas.key.mode=true \
                -Dspark.hadoop.fs.azure.sas.$containerName.$storageAccountName.blob.core.windows.net=$(cat /etc/secrets/${sasKeyName})";
            else
              export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
                -Dspark.hadoop.fs.azure.account.key.$storageAccountName.blob.core.windows.net=$storageAccountKey";
            fi;
          else
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=$logDirectory";
          fi;
          /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer;
        envFrom:
        - configMapRef:
            name: {{ include "spark-history-server.fullname" . }}
        livenessProbe:
          httpGet:
            path: /
            port: historyport
        readinessProbe:
          httpGet:
            path: /
            port: historyport
        {{- if .Values.pvc.enablePVC }}
        volumeMounts:
        - name: data
          mountPath: /data
        {{- else if .Values.hdfs.enableHDFS }}
        volumeMounts:
        - name: core-site
          mountPath: /etc/hadoop/core-site.xml
          subPath: core-site.xml
        - name: hdfs-site
          mountPath: /etc/hadoop/hdfs-site.xml
          subPath: hdfs-site.xml
        {{- end }}
      {{- if .Values.pvc.enablePVC }}
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: {{ .Values.pvc.existingClaimName }}
      {{- else if .Values.hdfs.enableHDFS }}
      volumes:
      - name: hdfs-site
        configMap:
          name: {{ .Values.hdfs.hdfsSiteConfigMap }}
      - name: core-site
        configMap:
          name: {{ .Values.hdfs.coreSiteConfigMap }}
      {{- end }}
    {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
    {{- end }}
